# [\#6476 Issue](https://github.com/ethereum/solidity/issues/6476) `closed`: [yul optimizer] Introducing additional scopes considered benign?
**Labels**: `optimizer`


#### <img src="https://avatars.githubusercontent.com/u/2388185?v=4" width="50">[bshastry](https://github.com/bshastry) opened issue at [2019-04-05 09:57](https://github.com/ethereum/solidity/issues/6476):

## Description

When we run the yul optimizer on the following program

```
{
        let a := 0
        for {
        }
        iszero(eq(a, 98))
        {
            a := add(a, 1)
        }
        {
            a := add(a, 1)
        }
}
```

... it gets "optimized" to

```
{
    {
        let a := 0
        for {
        }
        iszero(eq(a, 98))
        {
            a := add(a, 1)
        }
        {
            a := add(a, 1)
        }
    }
}
```

Bytecode generated for unoptimized and optimized code are exactly the same as expected
```
60005b606281141515601c576001810190505b6001810190506002565b50
60005b606281141515601c576001810190505b6001810190506002565b50
```

Note that the source code of the optimized version contains an additional nested scope (which is seemingly of little value). Of course, the two programs are semantically valid, so there is no cause for concern, but it tripped the fuzzer in a weird way.

### Tripping the fuzzer

We set an upper bound (`maxSteps`) on the number of basic blocks executed by the interpreter. This is to bail out of infinite loops. The exact bound is not important to this discussion, let's just call it `N`

If the yul optimizer can introduce at least one additional block, it can happen that the unoptimized version contains `N-1` blocks and does not exceed the interpreter's upper bound on num basic blocks executed. However, the optimized version reaches this limit because the optimizer has introduced an additional block.

This causes the traces to differ because of our assumption that "if the unoptimized code does not reach upper bound on number of basic blocks to be executed, then optimized code must not."

Since this assumption is seemingly untrue, what we could do is to unconditionally bail out if "maxSteps" is reached, whether in the unoptimized or optimized code.

#### <img src="https://avatars.githubusercontent.com/u/9073706?v=4" width="50">[chriseth](https://github.com/chriseth) commented at [2019-04-05 09:57](https://github.com/ethereum/solidity/issues/6476#issuecomment-480237241):

The optimizer can also unify blocks, inline functions, etc., so I think we cannot rely on those two bounds being the same anyway.

Does it make sense to set the bound to some value `N` when executing the non-optimized version and setting it to `N * 1.5` when executing the optimized version? This should make the assumption you name true again in most cases, and the cases where it fails should be inspected manually anyway, because it sounds like the optimizer did  a bad job for those.

#### <img src="https://avatars.githubusercontent.com/u/2388185?v=4" width="50">[bshastry](https://github.com/bshastry) commented at [2019-04-05 09:57](https://github.com/ethereum/solidity/issues/6476#issuecomment-480720299):

> Does it make sense to set the bound to some value N when executing the non-optimized version and setting it to N * 1.5 when executing the optimized version? This should make the assumption you name true again in most cases, and the cases where it fails should be inspected manually anyway, because it sounds like the optimizer did a bad job for those.

This makes sense, I took your feedback into account in the PR (#6491 ) to close this issue.


-------------------------------------------------------------------------------



[Export of Github issue for [ethereum/solidity](https://github.com/ethereum/solidity). Generated on 2022.05.23 at 03:51:38.]
