# [\#3515 Issue](https://github.com/ethereum/solidity/issues/3515) `closed`: Make `length` member read-only (S)
**Labels**: `breaking change :warning:`, `language design :rage4:`


#### <img src="https://avatars.githubusercontent.com/u/9073706?v=4" width="50">[chriseth](https://github.com/chriseth) opened issue at [2018-02-15 06:49](https://github.com/ethereum/solidity/issues/3515):

Once we have the `pop` member function (https://github.com/ethereum/solidity/issues/2780) there should be almost no need for writing to the `length` member of a dynamic storage array.

Arbitrary write access to the `length` member is dangerous as it can lead to gigantic arrays which are in turn vulnerable to a storage overlap attack (multiple arrays or an array and a mapping overlap in a predictable way). If many elements of various numbers are to be stored, mappings are probably superiour anyway.

#### <img src="https://avatars.githubusercontent.com/u/20340?v=4" width="50">[axic](https://github.com/axic) commented at [2018-02-16 10:03](https://github.com/ethereum/solidity/issues/3515#issuecomment-366192769):

Perhaps on certain cases there could be a `.truncate` function to drop multiple items, but if dropping is not constant on a given type I'd prefer not to have `.truncate`, but `.pop` only so it will be apparent to the programmer that gas usage must be considered (it will be quite clear with having `pop` in a `for` loop).

#### <img src="https://avatars.githubusercontent.com/u/9073706?v=4" width="50">[chriseth](https://github.com/chriseth) commented at [2018-02-16 11:06](https://github.com/ethereum/solidity/issues/3515#issuecomment-366206814):

Increasing the length currently is a constant-gas operation, but this exactly is the problem, it opens the door for overlap attacks.

Truncating is currently a linear-gas operation, so a for loop with pop calls would be roughly the same. There is a difference when you truncate an array with elements that are shorter than 16 bytes, because it only performs a single write per slot. A future optimizer might do loop unrolling to fix that, though.

#### <img src="https://avatars.githubusercontent.com/u/9073706?v=4" width="50">[chriseth](https://github.com/chriseth) commented at [2018-02-16 15:12](https://github.com/ethereum/solidity/issues/3515#issuecomment-366262110):

There are some use-cases where you want to extend the array by an element with its default zero value. Especially for more complex arrays, it is a detour to first construct it in memory and then `push`ing it onto the storage array.

For that, we should perhaps introduce `a.pushEmpty()`, which does exactly the same thing as `a.length++` (plus bounds check perhaps).

#### <img src="https://avatars.githubusercontent.com/u/799573?v=4" width="50">[wjmelements](https://github.com/wjmelements) commented at [2018-03-10 02:14](https://github.com/ethereum/solidity/issues/3515#issuecomment-371994395):

The best way to drop 5 elements at once is with a single SSTORE, subtracting 5 from the length. If length becomes read-only, we would need more builtins to restore lost functionality. 

For example, this: https://gist.github.com/wjmelements/fab8c6961cff3dbd8715c7c55d29d660
Produces cheaper code than: https://gist.github.com/wjmelements/a949117a87305b17482d2f517aa1c6e7

I am in support of `truncate`, and also `extend`, and would prefer to keep the ability to update arrays cheaply.

#### <img src="https://avatars.githubusercontent.com/u/9073706?v=4" width="50">[chriseth](https://github.com/chriseth) commented at [2018-03-12 14:03](https://github.com/ethereum/solidity/issues/3515#issuecomment-372320341):

truncate is not constant, but extend can be. Still, we want to prevent constant-cost resizing operations to avoid people having gigantic arrays that might cause collisions in storage.

#### <img src="https://avatars.githubusercontent.com/u/9073706?v=4" width="50">[chriseth](https://github.com/chriseth) commented at [2018-03-12 14:04](https://github.com/ethereum/solidity/issues/3515#issuecomment-372320663):

@wjmelements the simple examples you gave should compile to exactly the same code if the optimizer is used.

#### <img src="https://avatars.githubusercontent.com/u/799573?v=4" width="50">[wjmelements](https://github.com/wjmelements) commented at [2018-03-13 01:18](https://github.com/ethereum/solidity/issues/3515#issuecomment-372514030):

It's not the same code; with initial nonzero length, the first `sponsor` costs `128575` while the second `sponsor` costs `149929`. I suspect 20000 of that difference is four unnecessary SSTOREs on the length.

(compiled with optimization on 0.4.21+commit.dfe3193c.Emscripten.clang)

I would welcome the compiler performing such optimizations but at the moment it does not.

#### <img src="https://avatars.githubusercontent.com/u/799573?v=4" width="50">[wjmelements](https://github.com/wjmelements) commented at [2018-03-13 01:40](https://github.com/ethereum/solidity/issues/3515#issuecomment-372517796):

> Still, we want to prevent constant-cost resizing operations to avoid people having gigantic arrays that might cause collisions in storage.

Collisions are a [birthday problem](https://en.wikipedia.org/wiki/Birthday_problem), and not especially more likely with medium-sized arrays than with small ones. Very-large arrays are different of course.

```
contract HashCollision {
    uint256[] backdoor;
    mapping(address => uint256) balances;
    function HashCollision() {
        backdoor.length = 2 ** 130;
    }
    function useBackdoor(uint256 idx, uint256 val) external {
        backdoor[idx] = val;
    }
}
```

Higher gas costs are not a great way to protect against this. To protect devs from overextending I suggest adding a warning for nonconstant array extensions, and for constant array extensions larger than, say, 40.

#### <img src="https://avatars.githubusercontent.com/u/9073706?v=4" width="50">[chriseth](https://github.com/chriseth) commented at [2018-03-13 09:10](https://github.com/ethereum/solidity/issues/3515#issuecomment-372595766):

@wjmelements I think that calling it a birthday problem might be misleading, since in the traditional birthday problem, you have two random variables that collide. Here, you have two (pseudo)random variables, but can apply offsets. These offsets make it much easier to find collisions, since you don't really have to search.

Curiously, the reason why the optimizer does not reduce both versions to the same bytecode is again the potential for collisions: 

The `push` version writes data and increments the length multiple times, while the other version only increases the length at the end. The optimizer works correctly even in the case where the array data and the point where its length is stored overlap and thus takes into account that writing data might actually overwrite the length. Because of that, it always has to re-fetch the length and re-write it to storage. I created an issue for that: https://github.com/ethereum/solidity/issues/3716

#### <img src="https://avatars.githubusercontent.com/u/9073706?v=4" width="50">[chriseth](https://github.com/chriseth) commented at [2019-11-21 18:23](https://github.com/ethereum/solidity/issues/3515#issuecomment-557211149):

Implemented in #7350


-------------------------------------------------------------------------------



[Export of Github issue for [ethereum/solidity](https://github.com/ethereum/solidity). Generated on 2024.12.15 at 06:45:24.]
