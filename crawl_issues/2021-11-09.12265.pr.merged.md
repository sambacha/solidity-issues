# [\#12265 PR](https://github.com/ethereum/solidity/pull/12265) `merged`: Packed soljson.js

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) opened issue at [2021-11-09 17:08](https://github.com/ethereum/solidity/pull/12265):



#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2021-11-09 18:18](https://github.com/ethereum/solidity/pull/12265#issuecomment-964411917):

Passes all tests, that's a good sign.

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-07 14:34](https://github.com/ethereum/solidity/pull/12265#issuecomment-1007453733):

I now switched the PR to use LZ4 compression. As a result this is now 8MB instead of 6MB, which is still quite nice. I haven't measured decompression speed, but with a good LZ4 implementation it should be near-instant and ideally even vanish or result in a net-win when taking disc-loading time into account.

I took the decompression logic from https://github.com/Benzinga/lz4js, since it was easiest to strip of the compression logic and embed into a single function, but I'm not sure about its licensing (I asked about it in https://github.com/Benzinga/lz4js/issues/10) and also not sure about its speed.

So we may have to switch to a different implementation, but generally I'm quite happy with this and the reduction from >20MB to 8MB is quite nice.

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-10 11:51](https://github.com/ethereum/solidity/pull/12265#issuecomment-1008800645):

Performance-wise it would probably be best to use https://github.com/gorhill/lz4-wasm - but that'd be more involved: we'd need to measure the static overhead of wasm compilation vs. the faster runtime and we'd need to make sure wasm compilation is done in a portable manner (that works across browser and node, etc.), so might not be worth the trouble.

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-10 11:54](https://github.com/ethereum/solidity/pull/12265#issuecomment-1008802601):

Also: I don't exactly recall what the boundary for cloudfront to compress files on its own was - was it 10MB or so? If the 8MB we have as a result of this is below that threshold, we effectively add that on top of it as well, which would bring the size down to <6MB.

#### <img src="https://avatars.githubusercontent.com/u/137030?v=4" width="50">[cameel](https://github.com/cameel) commented at [2022-01-10 12:41](https://github.com/ethereum/solidity/pull/12265#issuecomment-1008837572):

> Also: I don't exactly recall what the boundary for cloudfront to compress files on its own was - was it 10MB or so?

Yeah, 10 MB. Here are the details from when I last checked that: https://github.com/ethereum/devops/issues/789#issuecomment-913732090.

> If the 8MB we have as a result of this is below that threshold, we effectively add that on top of it as well, which would bring the size down to <6MB.

I'm not sure I understand how that would work. Wouldn't that be compressing the file twice, which is usually not very effective? I.e. the LZ4 compressed binary would get gzipped in transit by the browser?

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-10 13:01](https://github.com/ethereum/solidity/pull/12265#issuecomment-1008851806):

> I'm not sure I understand how that would work. Wouldn't that be compressing the file twice, which is usually not very effective? I.e. the LZ4 compressed binary would get gzipped in transit by the browser?

Yes. LZ4 is very weak (but fast) compression, so it may still nicely compress further - also we have the overhead of base64 encoding the compressed data that allows further compression in transit. When I checked, the 8MB (lz4-base64-encoded) compressed down to <6MB if gzipped.

So we get good compression in transit, while decompression on an already downloaded file is probably around as fast as or even faster than loading more data from disk.

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-10 13:06](https://github.com/ethereum/solidity/pull/12265#issuecomment-1008855849):

Not sure we'll get a response about the licensing of the JS code I pulled in, though. Might be easier to just reimplement it - it's quite simple actually, especially since we only need decompression and not compression (yet I haven't seen another really simple dependency-free js implementation - https://github.com/pierrec/node-lz4 seems like quite the overkill).

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-17 10:15](https://github.com/ethereum/solidity/pull/12265#issuecomment-1014354059):

Yeah, maybe I won't get a response on the licensing of that lz4 code... we might rewrite something from scratch, maybe even fused with base64 decoding, although not sure if that'd work too well.

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-17 11:29](https://github.com/ethereum/solidity/pull/12265#issuecomment-1014419810):

I just did a crude measurement of the current version of the PR:

```
$ ln -sf uncompressed.js soljson.js
$ time { for i in $(seq 100); do ./test.js; done }

real    0m51,813s
user    1m46,384s
sys     0m16,339s
$ ln -sf compressed.js soljson.js
$ time { for i in $(seq 100); do ./test.js; done }

real    0m41,205s
user    1m46,055s
sys     0m6,874s

$ cat test.js
#!/usr/bin/node --wasm-dynamic-tiering
var Solidity = require('./soljson.js')
var x = Solidity.cwrap('solidity_version', 'string', [])()
```

That may even be skewed towards the uncompressed version, since I didn't account for disk caching or anything and the compressed version is still actually faster, so I'd say this is proof-of-concept and we can expect this to generally work out as planned.

As for sizes:
```
$ ls -l uncompressed.js 
-rw-r--r-- 1 daniel users 27351109 17. Jan 12:10 uncompressed.js
$ ls -l compressed.js 
-rw-r--r-- 1 daniel users 8370611 17. Jan 12:06 compressed.js
$ gzip compressed.js 
$ ls -l compressed.js.gz 
-rw-r--r-- 1 daniel users 5939434 17. Jan 12:06 compressed.js.gz
```

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-17 11:39](https://github.com/ethereum/solidity/pull/12265#issuecomment-1014427095):

I guess the actual speed-up is due to base64 decoding actually creating some overhead and this becomes less pronounced in the compressed version.

#### <img src="https://avatars.githubusercontent.com/u/137030?v=4" width="50">[cameel](https://github.com/cameel) commented at [2022-01-17 11:43](https://github.com/ethereum/solidity/pull/12265#issuecomment-1014430042):

Is base64 decoding here written in pure JS and compiled into the binary or is it some natively optimized node.js lib?

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-17 12:26](https://github.com/ethereum/solidity/pull/12265#issuecomment-1014467476):

> Is base64 decoding here written in pure JS and compiled into the binary or is it some natively optimized node.js lib?

It needs to work everywhere, so it's just some simple JS code.
If we let emscripten take care of the decoding, it apparently uses this (taken from a regular js-beautified soljson.js build):
```
var decodeBase64 = typeof atob === "function" ? atob : function(input) {
    var keyStr = "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=";
    var output = "";
    var chr1, chr2, chr3;
    var enc1, enc2, enc3, enc4;
    var i = 0;
    input = input.replace(/[^A-Za-z0-9\+\/\=]/g, "");
    do {
        enc1 = keyStr.indexOf(input.charAt(i++));
        enc2 = keyStr.indexOf(input.charAt(i++));
        enc3 = keyStr.indexOf(input.charAt(i++));
        enc4 = keyStr.indexOf(input.charAt(i++));
        chr1 = enc1 << 2 | enc2 >> 4;
        chr2 = (enc2 & 15) << 4 | enc3 >> 2;
        chr3 = (enc3 & 3) << 6 | enc4;
        output = output + String.fromCharCode(chr1);
        if (enc3 !== 64) {
            output = output + String.fromCharCode(chr2)
        }
        if (enc4 !== 64) {
            output = output + String.fromCharCode(chr3)
        }
    } while (i < input.length);
    return output
};

```
With the custom decompression logic in this PR I just used https://developer.mozilla.org/en-US/docs/Web/API/WindowBase64/Base64_encoding_and_decoding for now. Although it might make sense to just use the emscripten version as well, I just never bothered to extract it until now.

But I don't think there really is a much better way than doing this in plain JS.

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-17 12:28](https://github.com/ethereum/solidity/pull/12265#issuecomment-1014468967):

Ah, the emscripten version actually uses an ``atob`` builtin, if available - that probably makes sense, too - so yeah, this actually is another point that may skew things towards the uncompressed version, so we may even get a bit more out of the compressed version, if we improve that part.

``node`` does predefine ``atob`` and I guess it's natively optimized, so even more impressive that the compressed version *still* loads faster, even though I didn't use that so far :-).

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-17 13:12](https://github.com/ethereum/solidity/pull/12265#issuecomment-1014519542):

Haha, funnily the method emscripten uses for decoding base64 seems to be significantly slower than the pure JS snippet (not sure if that's due to ``atob`` being bad or whether it's due to ``atob`` returning a string and emscripten then copying the whole thing again to an ``Uint8Array``)... but I'll look into it a bit more to see what we can squeeze out of this :-). Although, I guess it doesn't matter *that* much, as long as the result is faster than what we had before anyways.

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-17 13:23](https://github.com/ethereum/solidity/pull/12265#issuecomment-1014533594):

Yeah... seems to me like the version I had in the PR, i.e. using the mozilla snippet to decode directly into an ``Uint8Array`` in pure JS is the fastest I can get, so I'll just stick with that.

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-01-17 13:39](https://github.com/ethereum/solidity/pull/12265#issuecomment-1014560119):

Actually using ``Buffer.from(..., 'base64')`` (which also seems to be native in ``node``, but directly produces a ``Uint8Array``-compatible buffer) is still faster by a bit...
```
$  time { for i in $(seq 100); do ./test.js; done }

real    0m36,670s
user    1m42,373s
sys     0m6,684s
```
So maybe I'll check if that's available and fall back to the mozilla snippet otherwise (doesn't look like there's a fast builtin mechanism for doing this in-browser and the snippet may be the way to go for that).

Funny in any case that the entire thing seems to be bound by base64 decoding and JS memory management (strings, buffers, arrays and stuff) rather than the decompression or disc loading or anything... That also means that it's probably unwise to switch to a stronger encoding like base85 or base91, since those decode even slower.

#### <img src="https://avatars.githubusercontent.com/u/137030?v=4" width="50">[cameel](https://github.com/cameel) commented at [2022-02-10 20:12](https://github.com/ethereum/solidity/pull/12265#issuecomment-1035449467):

By the way, do you think I should wait for this to be merged before rebuilding 0.5.17? Or would it be better to do it the old way?

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-02-10 20:20](https://github.com/ethereum/solidity/pull/12265#issuecomment-1035459481):

> By the way, do you think I should wait for this to be merged before rebuilding 0.5.17? Or would it be better to do it the old way?

Not overly important I'd say - fine either way. The big question is whether we want to rebuild and/or pack *all* wasm builds to save bandwidth at some point :-D. But that we'd need to coordinate with tooling anyways, since a change of old hashes has to be announced I guess.

#### <img src="https://avatars.githubusercontent.com/u/137030?v=4" width="50">[cameel](https://github.com/cameel) commented at [2022-02-10 20:37](https://github.com/ethereum/solidity/pull/12265#issuecomment-1035483889):

Instead of replacing the existing builds, we could add a new `emscripten-wasm32+lz4/` dir and put new rebuilds there. This would also have the advantage of leaving the old ones available in case something turns out to be wrong with the new ones.

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-02-10 20:43](https://github.com/ethereum/solidity/pull/12265#issuecomment-1035491585):

> Instead of replacing the existing builds, we could add a new `emscripten-wasm32+lz4/` dir and put new rebuilds there. This would also have the advantage of leaving the old ones available in case something turns out to be wrong with the new ones.

Yeah... we should wait a while and see if the compressed new releases cause any issues in any case.

We should also look into ipns for lists.json and pushing IPFS as properly supported distribution channel - with the self-decompression the fact that ipfs does not compress in transit is much less of an issue after this change :-) (even for proper ipfs loading like with js-ipfs - for gateways on the other hand transit compression might now also kick in)

#### <img src="https://avatars.githubusercontent.com/u/1347491?v=4" width="50">[ekpyron](https://github.com/ekpyron) commented at [2022-02-14 16:25](https://github.com/ethereum/solidity/pull/12265#issuecomment-1039287553):

By the way: these days remix mechanism for loading custom compilers works better than it used to.
I can load the current CI artifact https://974604-40892817-gh.circle-artifacts.com/0/soljson.js directly into remix and it works.
[Unfortunately I don't see an easy way for timing it objectively]


-------------------------------------------------------------------------------



[Export of Github issue for [ethereum/solidity](https://github.com/ethereum/solidity). Generated on 2024.12.15 at 06:45:24.]
