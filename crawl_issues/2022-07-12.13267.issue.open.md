# [\#13267 Issue](https://github.com/ethereum/solidity/issues/13267) `open`: No summary for our documentation pages in Google search results
**Labels**: `bug :bug:`, `help wanted`, `documentation :book:`, `low effort`, `low impact`


#### <img src="https://avatars.githubusercontent.com/u/137030?v=4" width="50">[cameel](https://github.com/cameel) opened issue at [2022-07-12 10:58](https://github.com/ethereum/solidity/issues/13267):

@chriseth reports that search hits in our docs look like this in Google:
![image](https://user-images.githubusercontent.com/137030/178474001-f26cf55c-76ca-4346-a7ef-ce3e407bd094.png)

Here's what Googles's help says about this: [No page information in search results](https://support.google.com/webmasters/answer/7489871).

I'm pretty sure this has something to do with the [`robots.txt`](https://docs.soliditylang.org/robots.txt) changes we did some time ago #10898. The search result seems to be from `develop`, which our `robots.txt` block. We only allow `latest`, `v0.7.6` and latest release. The question is - why is `develop` still getting indexed (and appears in results before those other allowed versions) if we blocked it?

#### <img src="https://avatars.githubusercontent.com/u/41555918?u=84f7000c0777aeaee5380622e748b6630a698922&v=4" width="50">[fewwwww](https://github.com/fewwwww) commented at [2022-07-16 03:07](https://github.com/ethereum/solidity/issues/13267#issuecomment-1186075592):

<img width="578" alt="截屏2022-07-15 20 06 58" src="https://user-images.githubusercontent.com/41555918/179336872-84ecdb3b-0eef-4205-a188-8cb321c6cc4b.png">
It works now. maybe it's a lagging in indexing?

#### <img src="https://avatars.githubusercontent.com/u/137030?v=4" width="50">[cameel](https://github.com/cameel) commented at [2022-07-18 09:12](https://github.com/ethereum/solidity/issues/13267#issuecomment-1186956514):

I still see this happening. For example searching for `solidity revert` I get this:

![solidity-revert-google-search-2022-07-18](https://user-images.githubusercontent.com/137030/179479547-2ee34564-f5e9-487b-95a8-c24983918d0c.png)

The upper result is from 0.8.13 while the lower one is for 0.8.15. I think we're seeing different results because Google has all versions still indexed and you can get hits from ones that are disallowed in `robots.txt` and ones that are allowed, depending on what you search for.

So the question would be how to stop Google from returning results from blocked versions and return newer ones instead. I wonder if it's actually possible - we assumed that blocking them in `robots.txt` would remove them from search completely but maybe that's not how it works.

#### <img src="https://avatars.githubusercontent.com/u/106779544?u=31c78ff02413561212829b789684292743bbd5d0&v=4" width="50">[imblue-dabadee](https://github.com/imblue-dabadee) commented at [2022-09-11 03:07](https://github.com/ethereum/solidity/issues/13267#issuecomment-1242875014):

`disallow` will stop Google from indexing/crawling the **contents** of the page, not the URL itself. This is why the URL for previous versions still appear but the meta description does not. [Reference from Google](https://developers.google.com/search/docs/advanced/robots/robots_txt#disallow)

To stop a URL being indexed by Google, you must add `<meta name="robots" content="noindex">` to relevant pages to assist with SEO. Funnily enough, you will have to redact the `disallow` field from the root robots.txt i.e. [this file](https://docs.soliditylang.org/robots.txt) to allow Google's web crawler to read the relevant tag s.t. that it will not index it [Reference from Google](https://developers.google.com/search/docs/crawling-indexing/block-indexing?hl=en). I noticed that there are robots.txt for each directory which will be ignored as Google or search engine crawlers only reference the root directory.

Keep in mind that once the `noindex` tag is added, it will not be removed from search results until it is crawled. To expedite that, using the URL Inspection tool from Google will allow you to request an index. [URL Inspection tool](https://support.google.com/webmasters/answer/9012289). 

Although the pages will be removed **after** Google's crawlers reach your page again, it does not necessarily mean the desired pages will be in their desired rankings after a search so keep that in mind for SEO. 

Hope that helps!

#### <img src="https://avatars.githubusercontent.com/u/137030?v=4" width="50">[cameel](https://github.com/cameel) commented at [2022-09-11 17:03](https://github.com/ethereum/solidity/issues/13267#issuecomment-1243003897):

Thanks! That explains a lot.

Not sure if we'll be able to keep those old version out of Google Search then since it might not be feasible to rebuild them. Especially if it would require changing code in the repo in already tagged releases.

In any case, pinging @r0qs since this is one of the topics, we'll want him to take over eventually.


-------------------------------------------------------------------------------



[Export of Github issue for [ethereum/solidity](https://github.com/ethereum/solidity). Generated on 2024.12.15 at 06:45:24.]
